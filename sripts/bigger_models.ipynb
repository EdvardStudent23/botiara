{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-12-15T07:41:01.310116Z","iopub.status.busy":"2025-12-15T07:41:01.309547Z","iopub.status.idle":"2025-12-15T08:07:18.676092Z","shell.execute_reply":"2025-12-15T08:07:18.675136Z","shell.execute_reply.started":"2025-12-15T07:41:01.310081Z"},"trusted":true},"outputs":[],"source":["\n","from datasets import load_dataset\n","\n","dataset = load_dataset(\n","    \"json\",\n","    data_files=\"/kaggle/input/trump-datasettt/trump_neutral_pairs.json\"\n",")[\"train\"]\n","\n","dataset = dataset.shuffle(seed=42)\n","\n","train_dataset = dataset.select(range(4500))\n","test_dataset  = dataset.select(range(4500, 5000))\n","\n","print(len(train_dataset), len(test_dataset))\n","\n","\n","def format_example(ex):\n","    return f\"Neutral: {ex['output']}\\nStyled: {ex['input']}\"\n","\n","train_dataset = train_dataset.map(lambda x: {\"text\": format_example(x)})\n","test_dataset  = test_dataset.map(lambda x: {\"text\": format_example(x)})\n","\n","\n","from transformers import AutoTokenizer\n","\n","MODEL_NAME = \"gpt2-medium\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","def tokenize(batch):\n","    return tokenizer(\n","        batch[\"text\"],\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=256\n","    )\n","\n","train_tok = train_dataset.map(tokenize, batched=True)\n","test_tok  = test_dataset.map(tokenize, batched=True)\n","\n","train_tok.set_format(\"torch\")\n","test_tok.set_format(\"torch\")\n","\n","train_tok = train_tok.remove_columns([\"input\", \"output\", \"text\"])\n","test_tok  = test_tok.remove_columns([\"input\", \"output\", \"text\"])\n","\n","\n","def add_labels(batch):\n","    batch[\"labels\"] = batch[\"input_ids\"].clone()\n","    return batch\n","\n","train_tok = train_tok.map(add_labels, batched=True)\n","test_tok  = test_tok.map(add_labels, batched=True)\n","\n","\n","from transformers import AutoModelForCausalLM\n","import torch\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\"\n",")\n","\n","model.config.use_cache = False\n","model.gradient_checkpointing_enable()\n","model.enable_input_require_grads()\n","\n","\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","lora_cfg = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    target_modules=[\"c_attn\", \"c_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.CAUSAL_LM\n",")\n","\n","model = get_peft_model(model, lora_cfg)\n","model.print_trainable_parameters()\n","\n","\n","from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./lora_trump_medium\",\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=8,\n","    warmup_steps=30,\n","    num_train_epochs=2,\n","    learning_rate=2e-4,\n","    fp16=False,\n","    logging_steps=20,\n","    save_steps=80,\n","    disable_tqdm=False,\n","    report_to=\"none\"\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_tok,\n","    eval_dataset=test_tok,\n","    tokenizer=tokenizer\n",")\n","\n","trainer.train()\n","\n","\n","neutral_file = \"/kaggle/input/trump-neutral-texts/neutral.txt\"\n","\n","with open(neutral_file, \"r\") as f:\n","    neutral_lines = [line.strip() for line in f if len(line.strip()) > 0]\n","\n","print(\"Loaded:\", len(neutral_lines))\n","\n","def stylize(text):\n","    prompt = f\"Neutral: {text}\\nStyled:\"\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    output = model.generate(\n","        **inputs,\n","        max_length=128,\n","        temperature=0.8,\n","        top_p=0.9,\n","        pad_token_id=tokenizer.eos_token_id\n","    )\n","\n","    out = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return out.split(\"Styled:\")[-1].strip()\n","\n","responses = [stylize(t) for t in neutral_lines]\n","\n","with open(\"responses.txt\", \"w\") as f:\n","    for r in responses:\n","        f.write(r + \"\\n\")\n","\n","print(\"Saved responses.txt\")\n","\n","\n","for i in range(10):\n","    neutral = test_dataset[i][\"output\"]\n","    real    = test_dataset[i][\"input\"]\n","    pred    = stylize(neutral)\n","\n","    print(\"Neutral:\", neutral)\n","    print(\"Pred:\", pred)\n","    print(\"Real:\", real)\n","    print(\"----------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-12-14T15:08:21.839594Z","iopub.status.busy":"2025-12-14T15:08:21.839294Z","iopub.status.idle":"2025-12-14T15:10:53.280376Z","shell.execute_reply":"2025-12-14T15:10:53.279702Z","shell.execute_reply.started":"2025-12-14T15:08:21.839573Z"},"trusted":true},"outputs":[],"source":["import json\n","import torch\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","\n","BASE_MODEL = \"gpt2-medium\"\n","LORA_PATH = \"/kaggle/working/lora_trump_medium/checkpoint-560\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","base = AutoModelForCausalLM.from_pretrained(\n","    BASE_MODEL,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","\n","model = PeftModel.from_pretrained(base, LORA_PATH)\n","model.eval()\n","\n","print(\"Model loaded\")\n","\n","dataset = load_dataset(\n","    \"json\",\n","    data_files=\"/kaggle/input/trump-datasettt/trump_neutral_pairs.json\"\n",")[\"train\"]\n","\n","print(\"Total records:\", len(dataset))\n","\n","test_dataset = dataset.select(range(len(dataset) - 100, len(dataset)))\n","\n","print(\"Test size:\", len(test_dataset))\n","\n","neutral_out = []\n","styled_out = []\n","\n","for ex in test_dataset:\n","    neutral = ex[\"output\"]\n","    prompt = f\"Neutral: {neutral}\\nStyled:\"\n","\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    with torch.no_grad():\n","        out = model.generate(\n","            **inputs,\n","            max_new_tokens=60,\n","            temperature=0.7,\n","            top_p=0.92\n","        )\n","\n","    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n","\n","    if \"Styled:\" in decoded:\n","        decoded = decoded.split(\"Styled:\", 1)[1].strip()\n","\n","    neutral_out.append(neutral)\n","    styled_out.append(decoded)\n","\n","with open(\"neutral_test.txt\", \"w\") as f:\n","    for line in neutral_out:\n","        f.write(line + \"\\n\")\n","\n","with open(\"styled_test_generated.txt\", \"w\") as f:\n","    for line in styled_out:\n","        f.write(line + \"\\n\")\n","\n","print(\"Saved neutral_test.txt and styled_test_generated.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-12-15T09:29:54.110347Z","iopub.status.busy":"2025-12-15T09:29:54.109743Z","iopub.status.idle":"2025-12-15T09:30:26.749074Z","shell.execute_reply":"2025-12-15T09:30:26.748145Z","shell.execute_reply.started":"2025-12-15T09:29:54.110324Z"},"trusted":true},"outputs":[],"source":["import torch\n","import json\n","import os\n","import math\n","from torch import nn\n","from safetensors.torch import load_file\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from openai import OpenAI\n","from peft import PeftModel\n","\n","client = OpenAI(api_key=\"your api key\")\n","\n","\n","def get_neutral_response(user_message):\n","    response = client.responses.create(\n","        model=\"gpt-5.2\",\n","        instructions=(\n","    \"You are a neutral assistant. \"\n","    \"Always respond with exactly one short sentence (max 12 words). \"\n","    \"Do not write more than one sentence.\"\n","),\n","        input=user_message\n","    )\n","    return response.output_text\n","\n","\n","BASE_MODEL = \"gpt2-medium\"\n","LORA_PATH = \"/kaggle/working/lora_trump_medium/checkpoint-560\"\n","\n","print(\"Loading tokenizer...\")\n","tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","print(\"Loading base model...\")\n","base = AutoModelForCausalLM.from_pretrained(\n","    BASE_MODEL,\n","    device_map=\"auto\"\n",")\n","\n","print(\"Loading LoRA...\")\n","model = PeftModel.from_pretrained(base, LORA_PATH)\n","model.eval()\n","\n","print(\"LoRA successfully loaded!\")\n","\n","\n","def stylize_trump(neutral_text):\n","    prompt = f\"Neutral: {neutral_text}\\nStyled:\"\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    with torch.no_grad():\n","        out = model.generate(\n","            **inputs,\n","            max_new_tokens=80,\n","            pad_token_id=tokenizer.eos_token_id,\n","            do_sample=False\n","        )\n","\n","    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n","    if \"Styled:\" in decoded:\n","        return decoded.split(\"Styled:\", 1)[1].strip()\n","    return decoded\n","\n","\n","print(\"\\n=== TRUMP CHAT (TRUE LoRA) READY ===\\n\")\n","\n","while True:\n","    user_msg = input(\"You: \").strip()\n","    if user_msg.lower() == \"exit\":\n","        break\n","\n","    neutral = get_neutral_response(user_msg)\n","    print(\"\\nNeutral response:\")\n","    print(neutral)\n","\n","    styled = stylize_trump(neutral)\n","    print(\"\\nTrump-style response:\")\n","    print(styled)\n","    print(\"----------------------------------------\\n\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":9013070,"sourceId":14142844,"sourceType":"datasetVersion"}],"dockerImageVersionId":31193,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":4}
